/**
 * ç»Ÿä¸€AI APIè°ƒç”¨æ¨¡å—
 * 
 * âš ï¸ å…¨é¡¹ç›®ä¸­ç¦æ­¢é‡å¤å†™ fetch / axios è°ƒç”¨ OpenAI / Gemini / Deepseek ç­‰æ¥å£ã€‚
 * æ‰€æœ‰AI APIè°ƒç”¨å¿…é¡»é€šè¿‡æ­¤æ¨¡å—è¿›è¡Œï¼Œç¡®ä¿ç»Ÿä¸€ç®¡ç†å’Œé”™è¯¯å¤„ç†ã€‚
 * 
 * âœ… ä½¿ç”¨ç»Ÿä¸€APIè¯·æ±‚æ¨¡å—ï¼Œç¦æ­¢ç›´æ¥ä½¿ç”¨fetch/axios
 * ğŸ“Œ æ‰€æœ‰APIåœ°å€ä»ç¯å¢ƒå˜é‡è·å–ï¼Œä¸¥ç¦ç¡¬ç¼–ç 
 */

import request from './request';
import { getAPIConfig } from './request';

/**
 * AIæ¨¡å‹ç±»å‹å®šä¹‰
 */
export type AIModel = 
  | 'gpt-4' | 'gpt-4-turbo' | 'gpt-3.5-turbo'
  | 'gemini-pro' | 'gemini-pro-vision'
  | 'deepseek-chat' | 'deepseek-coder'
  | 'qwen' | 'llama' | 'mistral'
  | 'claude-3' | 'claude-3-sonnet' | 'claude-3-haiku';

/**
 * å›¾åƒç”Ÿæˆæ¨¡å‹ç±»å‹å®šä¹‰
 */
export type ImageModel = 
  | 'dall-e-3' | 'dall-e-2' | 'midjourney'
  | 'stable-diffusion' | 'deepfloyd';

/**
 * AIè°ƒç”¨å‚æ•°æ¥å£
 */
export interface AICallParams {
  /** æç¤ºè¯ */
  prompt: string;
  /** æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨gpt-4 */
  model?: AIModel;
  /** æœ€å¤§tokenæ•° */
  maxTokens?: number;
  /** æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§ */
  temperature?: number;
  /** ç³»ç»Ÿæç¤ºè¯ */
  systemPrompt?: string;
  /** æ˜¯å¦æµå¼å“åº” */
  stream?: boolean;
  /** ç”¨æˆ·IDï¼Œç”¨äºæƒé™æ§åˆ¶ */
  userId?: string;
  /** é¢å¤–å‚æ•° */
  extraParams?: Record<string, any>;
}

/**
 * å›¾åƒç”Ÿæˆå‚æ•°æ¥å£
 */
export interface ImageGenerationParams {
  /** å›¾åƒæè¿°æç¤ºè¯ */
  prompt: string;
  /** å›¾åƒç”Ÿæˆæ¨¡å‹ */
  model?: ImageModel;
  /** ç”Ÿæˆå›¾åƒæ•°é‡ */
  n?: number;
  /** å›¾åƒå°ºå¯¸ */
  size?: '256x256' | '512x512' | '1024x1024' | '1792x1024' | '1024x1792';
  /** å“åº”æ ¼å¼ */
  responseFormat?: 'url' | 'b64_json';
  /** å‚è€ƒå›¾åƒï¼ˆbase64æ ¼å¼ï¼‰ */
  referenceImage?: string;
  /** ç”¨æˆ·IDï¼Œç”¨äºæƒé™æ§åˆ¶ */
  userId?: string;
}

/**
 * AIå“åº”æ¥å£
 */
export interface AIResponse {
  /** å“åº”å†…å®¹ */
  content: string;
  /** ä½¿ç”¨çš„æ¨¡å‹ */
  model: string;
  /** æ¶ˆè€—çš„tokenæ•° */
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  /** å“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ */
  responseTime: number;
  /** æ˜¯å¦æˆåŠŸ */
  success: boolean;
  /** é”™è¯¯ä¿¡æ¯ */
  error?: string;
}

/**
 * å›¾åƒç”Ÿæˆå“åº”æ¥å£
 */
export interface ImageGenerationResponse {
  /** ç”Ÿæˆçš„å›¾åƒåˆ—è¡¨ */
  images: Array<{
    url: string;
    revisedPrompt?: string;
  }>;
  /** ä½¿ç”¨çš„æ¨¡å‹ */
  model: string;
  /** å“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ */
  responseTime: number;
  /** æ˜¯å¦æˆåŠŸ */
  success: boolean;
  /** é”™è¯¯ä¿¡æ¯ */
  error?: string;
  /** åˆ›å»ºæ—¶é—´æˆ³ */
  created?: number;
}

/**
 * ç»Ÿä¸€çš„AI APIè°ƒç”¨å‡½æ•°
 * 
 * @param params AIè°ƒç”¨å‚æ•°
 * @returns AIå“åº”ç»“æœ
 * 
 * @example
 * ```typescript
 * // åŸºç¡€å¯¹è¯
 * const result = await callAI({
 *   prompt: "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
 *   model: "gpt-4"
 * });
 * 
 * // å¸¦ç³»ç»Ÿæç¤ºè¯çš„å¯¹è¯
 * const result = await callAI({
 *   prompt: "åˆ†æè¿™æ®µä»£ç çš„æ€§èƒ½é—®é¢˜",
 *   model: "gpt-4",
 *   systemPrompt: "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®¡æŸ¥ä¸“å®¶",
 *   temperature: 0.3
 * });
 * ```
 */
export async function callAI(params: AICallParams): Promise<AIResponse> {
  const startTime = Date.now();
  const {
    prompt,
    model = 'gpt-4',
    maxTokens = 1000,
    temperature = 0.7,
    systemPrompt,
    stream = false,
    userId,
    extraParams = {}
  } = params;

  try {
    // è·å–APIé…ç½®
    const apiConfig = getAPIConfig();

    // éªŒè¯é…ç½®
    if (!apiConfig.openai.apiKey || apiConfig.openai.apiKey.includes('{{') || apiConfig.openai.apiKey.includes('your-')) {
      throw new Error('OpenAI APIå¯†é’¥æœªæ­£ç¡®é…ç½®ï¼Œè¯·åœ¨.env.localæ–‡ä»¶ä¸­è®¾ç½®VITE_OPENAI_API_KEY');
    }

    // æ„å»ºè¯·æ±‚ä½“
    const requestBody: any = {
      model: getModelMapping(model),
      messages: [
        ...(systemPrompt ? [{ role: 'system', content: systemPrompt }] : []),
        { role: 'user', content: prompt }
      ],
      max_tokens: maxTokens,
      temperature,
      stream,
      ...extraParams
    };

    // æ·»åŠ ç”¨æˆ·ä¿¡æ¯ï¼ˆå¦‚æœæä¾›ï¼‰
    if (userId) {
      requestBody.user = userId;
    }

    // ä½¿ç”¨ç»Ÿä¸€è¯·æ±‚æ¨¡å—å‘é€è¯·æ±‚
    const data = await request.post('/chat/completions', requestBody, {
      baseURL: apiConfig.openai.baseURL,
      headers: {
        'Authorization': `Bearer ${apiConfig.openai.apiKey}`,
        ...(userId && { 'X-User-ID': userId })
      }
    });

    // å¤„ç†æµå¼å“åº”
    if (stream) {
      return handleStreamResponse(data, model, startTime);
    }

    // å¤„ç†æ™®é€šå“åº”
    const content = data.choices[0]?.message?.content || '';
    const usage = data.usage;

    return {
      content,
      model,
      usage,
      responseTime: Date.now() - startTime,
      success: true
    };

  } catch (error) {
    console.error('AI APIè°ƒç”¨å¤±è´¥:', error);
    
    return {
      content: '',
      model,
      responseTime: Date.now() - startTime,
      success: false,
      error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'
    };
  }
}

/**
 * å¤„ç†æµå¼å“åº”
 */
async function handleStreamResponse(
  data: any, 
  model: string, 
  startTime: number
): Promise<AIResponse> {
  let content = '';

  try {
    for (const chunk of data.choices) {
      if (chunk.delta?.content) {
        content += chunk.delta.content;
      }
    }

    return {
      content,
      model,
      responseTime: Date.now() - startTime,
      success: true
    };

  } catch (e) {
    console.error('æµå¼å“åº”å¤„ç†å¤±è´¥:', e);
    return {
      content,
      model,
      responseTime: Date.now() - startTime,
      success: false,
      error: 'æµå¼å“åº”å¤„ç†å¤±è´¥'
    };
  }
}

/**
 * æ¨¡å‹åç§°æ˜ å°„
 */
function getModelMapping(model: AIModel): string {
  const modelMap: Record<AIModel, string> = {
    'gpt-4': 'gpt-4',
    'gpt-4-turbo': 'gpt-4-1106-preview',
    'gpt-3.5-turbo': 'gpt-3.5-turbo',
    'gemini-pro': 'gemini-pro',
    'gemini-pro-vision': 'gemini-pro-vision',
    'deepseek-chat': 'deepseek-chat',
    'deepseek-coder': 'deepseek-coder',
    'qwen': 'qwen-turbo',
    'llama': 'llama-2-70b-chat',
    'mistral': 'mistral-7b-instruct',
    'claude-3': 'claude-3-opus-20240229',
    'claude-3-sonnet': 'claude-3-sonnet-20240229',
    'claude-3-haiku': 'claude-3-haiku-20240307'
  };

  return modelMap[model] || model;
}

/**
 * ç»Ÿä¸€çš„å›¾åƒç”ŸæˆAPIè°ƒç”¨å‡½æ•°
 * 
 * @param params å›¾åƒç”Ÿæˆå‚æ•°
 * @returns å›¾åƒç”Ÿæˆå“åº”ç»“æœ
 * 
 * @example
 * ```typescript
 * // åŸºç¡€å›¾åƒç”Ÿæˆ
 * const result = await generateImage({
 *   prompt: "ä¸€åªå¯çˆ±çš„å°çŒ«ååœ¨èŠ±å›­é‡Œ",
 *   model: "dall-e-3",
 *   size: "1024x1024"
 * });
 * 
 * // å¸¦å‚è€ƒå›¾åƒçš„å˜ä½“ç”Ÿæˆ
 * const result = await generateImage({
 *   prompt: "å°†è¿™ä¸ªå›¾åƒå˜æˆæ°´å½©ç”»é£æ ¼",
 *   model: "dall-e-3",
 *   referenceImage: "data:image/jpeg;base64,..."
 * });
 * ```
 */
export async function generateImage(params: ImageGenerationParams): Promise<ImageGenerationResponse> {
  const startTime = Date.now();
  const {
    prompt,
    model = 'dall-e-3',
    n = 1,
    size = '1024x1024',
    responseFormat = 'url',
    referenceImage,
    userId
  } = params;

  try {
    // è·å–APIé…ç½®
    const apiConfig = getAPIConfig();

    // éªŒè¯é…ç½®
    if (!apiConfig.openai.apiKey || apiConfig.openai.apiKey.includes('{{') || apiConfig.openai.apiKey.includes('your-')) {
      throw new Error('OpenAI APIå¯†é’¥æœªæ­£ç¡®é…ç½®ï¼Œè¯·åœ¨.env.localæ–‡ä»¶ä¸­è®¾ç½®VITE_OPENAI_API_KEY');
    }

    // æ„å»ºè¯·æ±‚ä½“
    const requestBody: any = {
      model: getImageModelMapping(model),
      prompt,
      n,
      size,
      response_format: responseFormat
    };

    // å¦‚æœæœ‰å‚è€ƒå›¾åƒï¼Œä½¿ç”¨DALL-E 3çš„å˜ä½“åŠŸèƒ½
    if (referenceImage) {
      requestBody.image = referenceImage;
      requestBody.model = 'dall-e-3';
    }

    // æ„å»ºè¯·æ±‚å¤´
    const headers: Record<string, string> = {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiConfig.openai.apiKey}`
    };

    // æ·»åŠ ç”¨æˆ·ä¿¡æ¯ï¼ˆå¦‚æœæä¾›ï¼‰
    if (userId) {
      headers['X-User-ID'] = userId;
    }

    // ä½¿ç”¨ç»Ÿä¸€è¯·æ±‚æ¨¡å—å‘é€è¯·æ±‚
    const data = await request.post('/v1/images/generations', requestBody, {
      baseURL: apiConfig.openai.baseURL,
      headers
    });

    const images = data.data.map((item: any) => ({
      url: item.url,
      revisedPrompt: item.revised_prompt
    }));

    return {
      images,
      model,
      responseTime: Date.now() - startTime,
      success: true,
      created: data.created
    };

  } catch (error) {
    console.error('å›¾åƒç”ŸæˆAPIè°ƒç”¨å¤±è´¥:', error);
    
    return {
      images: [],
      model,
      responseTime: Date.now() - startTime,
      success: false,
      error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'
    };
  }
}

/**
 * å›¾åƒæ¨¡å‹åç§°æ˜ å°„
 */
function getImageModelMapping(model: ImageModel): string {
  const modelMap: Record<ImageModel, string> = {
    'dall-e-3': 'dall-e-3',
    'dall-e-2': 'dall-e-2',
    'midjourney': 'midjourney',
    'stable-diffusion': 'stable-diffusion-xl',
    'deepfloyd': 'deepfloyd-if'
  };

  return modelMap[model] || model;
}

/**
 * æ‰¹é‡AIè°ƒç”¨
 * 
 * @param prompts æç¤ºè¯æ•°ç»„
 * @param params é€šç”¨å‚æ•°
 * @returns å“åº”ç»“æœæ•°ç»„
 */
export async function callAIBatch(
  prompts: string[], 
  params: Omit<AICallParams, 'prompt'> = {}
): Promise<AIResponse[]> {
  const results: AIResponse[] = [];
  
  for (const prompt of prompts) {
    const result = await callAI({ ...params, prompt });
    results.push(result);
  }
  
  return results;
}

/**
 * å¸¦é‡è¯•çš„AIè°ƒç”¨
 * 
 * @param params AIè°ƒç”¨å‚æ•°
 * @param maxRetries æœ€å¤§é‡è¯•æ¬¡æ•°
 * @returns AIå“åº”ç»“æœ
 */
export async function callAIWithRetry(
  params: AICallParams, 
  maxRetries: number = 3
): Promise<AIResponse> {
  let lastError: Error | null = null;
  
  for (let i = 0; i < maxRetries; i++) {
    try {
      const result = await callAI(params);
      
      if (result.success) {
        return result;
      }
      
      lastError = new Error(result.error || 'è°ƒç”¨å¤±è´¥');
      
    } catch (error) {
      lastError = error instanceof Error ? error : new Error('æœªçŸ¥é”™è¯¯');
    }
    
    // ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
    if (i < maxRetries - 1) {
      await new Promise(resolve => setTimeout(resolve, Math.pow(2, i) * 1000));
    }
  }
  
  throw lastError || new Error('æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†');
}

/**
 * æ£€æŸ¥AIæœåŠ¡çŠ¶æ€
 * 
 * @returns æœåŠ¡çŠ¶æ€ä¿¡æ¯
 */
export async function checkAIStatus(): Promise<{
  openai: boolean;
  gemini: boolean;
  deepseek: boolean;
  message: string;
}> {
  try {
    const result = await callAI({
      prompt: 'Hello',
      model: 'gpt-3.5-turbo',
      maxTokens: 10
    });

    return {
      openai: result.success,
      gemini: false, // éœ€è¦å•ç‹¬æµ‹è¯•
      deepseek: false, // éœ€è¦å•ç‹¬æµ‹è¯•
      message: result.success ? 'AIæœåŠ¡æ­£å¸¸' : 'AIæœåŠ¡å¼‚å¸¸'
    };
  } catch (error) {
    return {
      openai: false,
      gemini: false,
      deepseek: false,
      message: `AIæœåŠ¡æ£€æŸ¥å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`
    };
  }
}

/**
 * è·å–AIæ¨¡å‹åˆ—è¡¨
 * 
 * @returns å¯ç”¨æ¨¡å‹åˆ—è¡¨
 */
export function getAvailableModels(): AIModel[] {
  return [
    'gpt-4',
    'gpt-4-turbo', 
    'gpt-3.5-turbo',
    'gemini-pro',
    'deepseek-chat',
    'claude-3',
    'qwen',
    'llama',
    'mistral'
  ];
}

/**
 * ä¼°ç®—AIè°ƒç”¨æˆæœ¬
 * 
 * @param prompt æç¤ºè¯
 * @param model æ¨¡å‹
 * @returns ä¼°ç®—æˆæœ¬ï¼ˆç¾å…ƒï¼‰
 */
export function estimateAICost(prompt: string, model: AIModel = 'gpt-4'): number {
  const promptTokens = Math.ceil(prompt.length / 4); // ç²—ç•¥ä¼°ç®—
  const completionTokens = Math.ceil(promptTokens * 0.5); // å‡è®¾å›å¤é•¿åº¦æ˜¯æç¤ºçš„ä¸€åŠ
  
  const costPer1kTokens = {
    'gpt-4': 0.03,
    'gpt-4-turbo': 0.01,
    'gpt-3.5-turbo': 0.002,
    'gemini-pro': 0.001,
    'deepseek-chat': 0.002,
    'claude-3': 0.015,
    'qwen': 0.001,
    'llama': 0.001,
    'mistral': 0.001
  };
  
  const cost = costPer1kTokens[model] || 0.01;
  return (promptTokens + completionTokens) * cost / 1000;
} 