/**
 * âœ… æœ¬æ–‡ä»¶å°è£…æ‰€æœ‰ AI API çš„è°ƒç”¨é€»è¾‘ã€‚
 * âš ï¸ å…¨é¡¹ç›®ä¸­ç¦æ­¢é‡å¤å†™ fetch / axios è°ƒç”¨ OpenAI / Gemini / Deepseek ç­‰æ¥å£ã€‚
 * ğŸš« è¯·ç»Ÿä¸€ä½¿ç”¨ callAI() æ–¹æ³•ã€‚
 * 
 * æ”¯æŒçš„æ¨¡å‹ï¼š
 * - OpenAI: gpt-4, gpt-4-turbo, gpt-3.5-turbo
 * - Gemini: gemini-pro, gemini-pro-vision
 * - Deepseek: deepseek-chat, deepseek-coder
 * - æœ¬åœ°æ¨¡å‹: qwen, llama, mistral
 * - å›¾åƒç”Ÿæˆ: dall-e-3, dall-e-2, midjourney
 */

import { getAPIConfig } from '@/config/apiConfig';
import { getAuthingConfig } from '@/config/authing';

/**
 * AIæ¨¡å‹ç±»å‹
 */
export type AIModel = 
  | 'gpt-4' | 'gpt-4-turbo' | 'gpt-3.5-turbo'
  | 'gemini-pro' | 'gemini-pro-vision'
  | 'deepseek-chat' | 'deepseek-coder'
  | 'qwen' | 'llama' | 'mistral'
  | 'claude-3' | 'claude-3-sonnet' | 'claude-3-haiku';

/**
 * å›¾åƒç”Ÿæˆæ¨¡å‹ç±»å‹
 */
export type ImageModel = 
  | 'dall-e-3' | 'dall-e-2' | 'midjourney'
  | 'stable-diffusion' | 'deepfloyd';

/**
 * AIè°ƒç”¨å‚æ•°
 */
export interface AICallParams {
  /** æç¤ºè¯ */
  prompt: string;
  /** æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨gpt-4 */
  model?: AIModel;
  /** æœ€å¤§tokenæ•° */
  maxTokens?: number;
  /** æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§ */
  temperature?: number;
  /** ç³»ç»Ÿæç¤ºè¯ */
  systemPrompt?: string;
  /** æ˜¯å¦æµå¼å“åº” */
  stream?: boolean;
  /** ç”¨æˆ·IDï¼Œç”¨äºæƒé™æ§åˆ¶ */
  userId?: string;
  /** é¢å¤–å‚æ•° */
  extraParams?: Record<string, any>;
}

/**
 * å›¾åƒç”Ÿæˆå‚æ•°
 */
export interface ImageGenerationParams {
  /** å›¾åƒæè¿°æç¤ºè¯ */
  prompt: string;
  /** å›¾åƒç”Ÿæˆæ¨¡å‹ */
  model?: ImageModel;
  /** ç”Ÿæˆå›¾åƒæ•°é‡ */
  n?: number;
  /** å›¾åƒå°ºå¯¸ */
  size?: '256x256' | '512x512' | '1024x1024' | '1792x1024' | '1024x1792';
  /** å“åº”æ ¼å¼ */
  responseFormat?: 'url' | 'b64_json';
  /** å‚è€ƒå›¾åƒï¼ˆbase64æ ¼å¼ï¼‰ */
  referenceImage?: string;
  /** ç”¨æˆ·IDï¼Œç”¨äºæƒé™æ§åˆ¶ */
  userId?: string;
}

/**
 * AIå“åº”ç»“æœ
 */
export interface AIResponse {
  /** å“åº”å†…å®¹ */
  content: string;
  /** ä½¿ç”¨çš„æ¨¡å‹ */
  model: string;
  /** æ¶ˆè€—çš„tokenæ•° */
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  /** å“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ */
  responseTime: number;
  /** æ˜¯å¦æˆåŠŸ */
  success: boolean;
  /** é”™è¯¯ä¿¡æ¯ */
  error?: string;
}

/**
 * å›¾åƒç”Ÿæˆå“åº”ç»“æœ
 */
export interface ImageGenerationResponse {
  /** ç”Ÿæˆçš„å›¾åƒåˆ—è¡¨ */
  images: Array<{
    url: string;
    revisedPrompt?: string;
  }>;
  /** ä½¿ç”¨çš„æ¨¡å‹ */
  model: string;
  /** å“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ */
  responseTime: number;
  /** æ˜¯å¦æˆåŠŸ */
  success: boolean;
  /** é”™è¯¯ä¿¡æ¯ */
  error?: string;
  /** åˆ›å»ºæ—¶é—´æˆ³ */
  created?: number;
}

/**
 * ç»Ÿä¸€çš„AI APIè°ƒç”¨å‡½æ•°
 * 
 * @param params AIè°ƒç”¨å‚æ•°
 * @returns AIå“åº”ç»“æœ
 * 
 * @example
 * ```typescript
 * // åŸºç¡€è°ƒç”¨
 * const result = await callAI({
 *   prompt: "è¯·å¸®æˆ‘å†™ä¸€ä¸ªReactç»„ä»¶",
 *   model: "gpt-4"
 * });
 * 
 * // å¸¦ç³»ç»Ÿæç¤ºçš„è°ƒç”¨
 * const result = await callAI({
 *   prompt: "åˆ†æè¿™æ®µä»£ç ",
 *   systemPrompt: "ä½ æ˜¯ä¸€ä¸ªä»£ç å®¡æŸ¥ä¸“å®¶",
 *   temperature: 0.3
 * });
 * ```
 */
export async function callAI(params: AICallParams): Promise<AIResponse> {
  const startTime = Date.now();
  const {
    prompt,
    model = 'gpt-4',
    maxTokens = 2000,
    temperature = 0.7,
    systemPrompt,
    stream = false,
    userId,
    extraParams = {}
  } = params;

  try {
    // è·å–APIé…ç½®
    const apiConfig = getAPIConfig();
    const authingConfig = getAuthingConfig();

    // éªŒè¯é…ç½®
    if (!apiConfig.openai.apiKey) {
      throw new Error('OpenAI APIå¯†é’¥æœªé…ç½®');
    }

    // æ„å»ºè¯·æ±‚ä½“
    const messages = [];
    
    // æ·»åŠ ç³»ç»Ÿæç¤º
    if (systemPrompt) {
      messages.push({
        role: 'system',
        content: systemPrompt
      });
    }

    // æ·»åŠ ç”¨æˆ·æç¤º
    messages.push({
      role: 'user',
      content: prompt
    });

    const requestBody = {
      model: getModelMapping(model),
      messages,
      max_tokens: maxTokens,
      temperature,
      stream,
      ...extraParams
    };

    // æ„å»ºè¯·æ±‚å¤´
    const headers: Record<string, string> = {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiConfig.openai.apiKey}`
    };

    // æ·»åŠ ç”¨æˆ·ä¿¡æ¯ï¼ˆå¦‚æœæä¾›ï¼‰
    if (userId) {
      headers['X-User-ID'] = userId;
    }

    // å‘é€è¯·æ±‚
    const response = await fetch(apiConfig.openai.baseURL + '/v1/chat/completions', {
      method: 'POST',
      headers,
      body: JSON.stringify(requestBody)
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw new Error(`APIè¯·æ±‚å¤±è´¥: ${response.status} ${response.statusText} - ${errorData.error?.message || 'æœªçŸ¥é”™è¯¯'}`);
    }

    // å¤„ç†æµå¼å“åº”
    if (stream) {
      return handleStreamResponse(response, model, startTime);
    }

    // å¤„ç†æ™®é€šå“åº”
    const data = await response.json();
    const content = data.choices[0]?.message?.content || '';
    const usage = data.usage;

    return {
      content,
      model,
      usage,
      responseTime: Date.now() - startTime,
      success: true
    };

  } catch (error) {
    console.error('AI APIè°ƒç”¨å¤±è´¥:', error);
    
    return {
      content: '',
      model,
      responseTime: Date.now() - startTime,
      success: false,
      error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'
    };
  }
}

/**
 * å¤„ç†æµå¼å“åº”
 */
async function handleStreamResponse(
  response: Response, 
  model: string, 
  startTime: number
): Promise<AIResponse> {
  const reader = response.body?.getReader();
  if (!reader) {
    throw new Error('æ— æ³•è¯»å–æµå¼å“åº”');
  }

  let content = '';
  const decoder = new TextDecoder();

  try {
    while (true) {
      const { done, value } = await reader.read();
      
      if (done) break;

      const chunk = decoder.decode(value);
      const lines = chunk.split('\n');

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.slice(6);
          
          if (data === '[DONE]') {
            return {
              content,
              model,
              responseTime: Date.now() - startTime,
              success: true
            };
          }

          try {
            const parsed = JSON.parse(data);
            const delta = parsed.choices[0]?.delta?.content;
            if (delta) {
              content += delta;
            }
          } catch (e) {
            // å¿½ç•¥è§£æé”™è¯¯
          }
        }
      }
    }

    return {
      content,
      model,
      responseTime: Date.now() - startTime,
      success: true
    };

  } finally {
    reader.releaseLock();
  }
}

/**
 * æ¨¡å‹åç§°æ˜ å°„
 */
function getModelMapping(model: AIModel): string {
  const modelMap: Record<AIModel, string> = {
    'gpt-4': 'gpt-4',
    'gpt-4-turbo': 'gpt-4-1106-preview',
    'gpt-3.5-turbo': 'gpt-3.5-turbo',
    'gemini-pro': 'gemini-pro',
    'gemini-pro-vision': 'gemini-pro-vision',
    'deepseek-chat': 'deepseek-chat',
    'deepseek-coder': 'deepseek-coder',
    'qwen': 'qwen-turbo',
    'llama': 'llama-2-70b-chat',
    'mistral': 'mistral-7b-instruct',
    'claude-3': 'claude-3-opus-20240229',
    'claude-3-sonnet': 'claude-3-sonnet-20240229',
    'claude-3-haiku': 'claude-3-haiku-20240307'
  };

  return modelMap[model] || model;
}

/**
 * ç»Ÿä¸€çš„å›¾åƒç”ŸæˆAPIè°ƒç”¨å‡½æ•°
 * 
 * @param params å›¾åƒç”Ÿæˆå‚æ•°
 * @returns å›¾åƒç”Ÿæˆå“åº”ç»“æœ
 * 
 * @example
 * ```typescript
 * // åŸºç¡€å›¾åƒç”Ÿæˆ
 * const result = await generateImage({
 *   prompt: "ä¸€åªå¯çˆ±çš„å°çŒ«ååœ¨èŠ±å›­é‡Œ",
 *   model: "dall-e-3",
 *   size: "1024x1024"
 * });
 * 
 * // å¸¦å‚è€ƒå›¾åƒçš„å˜ä½“ç”Ÿæˆ
 * const result = await generateImage({
 *   prompt: "å°†è¿™ä¸ªå›¾åƒå˜æˆæ°´å½©ç”»é£æ ¼",
 *   model: "dall-e-3",
 *   referenceImage: "data:image/jpeg;base64,..."
 * });
 * ```
 */
export async function generateImage(params: ImageGenerationParams): Promise<ImageGenerationResponse> {
  const startTime = Date.now();
  const {
    prompt,
    model = 'dall-e-3',
    n = 1,
    size = '1024x1024',
    responseFormat = 'url',
    referenceImage,
    userId
  } = params;

  try {
    // è·å–APIé…ç½®
    const apiConfig = getAPIConfig();

    // éªŒè¯é…ç½®
    if (!apiConfig.openai.apiKey) {
      throw new Error('OpenAI APIå¯†é’¥æœªé…ç½®');
    }

    // æ„å»ºè¯·æ±‚ä½“
    const requestBody: any = {
      model: getImageModelMapping(model),
      prompt,
      n,
      size,
      response_format: responseFormat
    };

    // å¦‚æœæœ‰å‚è€ƒå›¾åƒï¼Œä½¿ç”¨DALL-E 3çš„å˜ä½“åŠŸèƒ½
    if (referenceImage) {
      requestBody.image = referenceImage;
      requestBody.model = 'dall-e-3';
    }

    // æ„å»ºè¯·æ±‚å¤´
    const headers: Record<string, string> = {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiConfig.openai.apiKey}`
    };

    // æ·»åŠ ç”¨æˆ·ä¿¡æ¯ï¼ˆå¦‚æœæä¾›ï¼‰
    if (userId) {
      headers['X-User-ID'] = userId;
    }

    // å‘é€è¯·æ±‚
    const response = await fetch(apiConfig.openai.baseURL + '/v1/images/generations', {
      method: 'POST',
      headers,
      body: JSON.stringify(requestBody)
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw new Error(`å›¾åƒç”ŸæˆAPIè¯·æ±‚å¤±è´¥: ${response.status} ${response.statusText} - ${errorData.error?.message || 'æœªçŸ¥é”™è¯¯'}`);
    }

    const data = await response.json();
    
    return {
      images: data.data.map((item: any) => ({
        url: item.url,
        revisedPrompt: item.revised_prompt
      })),
      model,
      responseTime: Date.now() - startTime,
      success: true,
      created: data.created
    };

  } catch (error) {
    console.error('å›¾åƒç”ŸæˆAPIè°ƒç”¨å¤±è´¥:', error);
    
    return {
      images: [],
      model,
      responseTime: Date.now() - startTime,
      success: false,
      error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'
    };
  }
}

/**
 * å›¾åƒæ¨¡å‹åç§°æ˜ å°„
 */
function getImageModelMapping(model: ImageModel): string {
  const modelMap: Record<ImageModel, string> = {
    'dall-e-3': 'dall-e-3',
    'dall-e-2': 'dall-e-2',
    'midjourney': 'midjourney',
    'stable-diffusion': 'stable-diffusion-xl',
    'deepfloyd': 'deepfloyd-if'
  };

  return modelMap[model] || model;
}

/**
 * æ‰¹é‡AIè°ƒç”¨
 * 
 * @param prompts æç¤ºè¯æ•°ç»„
 * @param params é€šç”¨å‚æ•°
 * @returns å“åº”ç»“æœæ•°ç»„
 */
export async function callAIBatch(
  prompts: string[], 
  params: Omit<AICallParams, 'prompt'> = {}
): Promise<AIResponse[]> {
  const results: AIResponse[] = [];
  
  for (const prompt of prompts) {
    const result = await callAI({ ...params, prompt });
    results.push(result);
  }
  
  return results;
}

/**
 * å¸¦é‡è¯•çš„AIè°ƒç”¨
 * 
 * @param params AIè°ƒç”¨å‚æ•°
 * @param maxRetries æœ€å¤§é‡è¯•æ¬¡æ•°
 * @returns AIå“åº”ç»“æœ
 */
export async function callAIWithRetry(
  params: AICallParams, 
  maxRetries: number = 3
): Promise<AIResponse> {
  let lastError: Error | null = null;
  
  for (let i = 0; i < maxRetries; i++) {
    try {
      const result = await callAI(params);
      
      if (result.success) {
        return result;
      }
      
      lastError = new Error(result.error || 'è°ƒç”¨å¤±è´¥');
      
    } catch (error) {
      lastError = error instanceof Error ? error : new Error('æœªçŸ¥é”™è¯¯');
    }
    
    // ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
    if (i < maxRetries - 1) {
      await new Promise(resolve => setTimeout(resolve, Math.pow(2, i) * 1000));
    }
  }
  
  throw lastError || new Error('æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†');
}

/**
 * æ£€æŸ¥AIæœåŠ¡çŠ¶æ€
 * 
 * @returns æœåŠ¡çŠ¶æ€ä¿¡æ¯
 */
export async function checkAIStatus(): Promise<{
  openai: boolean;
  gemini: boolean;
  deepseek: boolean;
  message: string;
}> {
  try {
    const result = await callAI({
      prompt: 'Hello',
      model: 'gpt-3.5-turbo',
      maxTokens: 10
    });

    return {
      openai: result.success,
      gemini: false, // éœ€è¦å•ç‹¬æµ‹è¯•
      deepseek: false, // éœ€è¦å•ç‹¬æµ‹è¯•
      message: result.success ? 'AIæœåŠ¡æ­£å¸¸' : 'AIæœåŠ¡å¼‚å¸¸'
    };
  } catch (error) {
    return {
      openai: false,
      gemini: false,
      deepseek: false,
      message: `AIæœåŠ¡æ£€æŸ¥å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`
    };
  }
}

/**
 * è·å–AIæ¨¡å‹åˆ—è¡¨
 * 
 * @returns å¯ç”¨æ¨¡å‹åˆ—è¡¨
 */
export function getAvailableModels(): AIModel[] {
  return [
    'gpt-4',
    'gpt-4-turbo', 
    'gpt-3.5-turbo',
    'gemini-pro',
    'deepseek-chat',
    'claude-3',
    'qwen',
    'llama',
    'mistral'
  ];
}

/**
 * ä¼°ç®—AIè°ƒç”¨æˆæœ¬
 * 
 * @param prompt æç¤ºè¯
 * @param model æ¨¡å‹
 * @returns ä¼°ç®—æˆæœ¬ï¼ˆç¾å…ƒï¼‰
 */
export function estimateAICost(prompt: string, model: AIModel = 'gpt-4'): number {
  const promptTokens = Math.ceil(prompt.length / 4); // ç²—ç•¥ä¼°ç®—
  const completionTokens = Math.ceil(promptTokens * 0.5); // å‡è®¾å›å¤é•¿åº¦æ˜¯æç¤ºçš„ä¸€åŠ
  
  const costPer1kTokens = {
    'gpt-4': 0.03,
    'gpt-4-turbo': 0.01,
    'gpt-3.5-turbo': 0.002,
    'gemini-pro': 0.001,
    'deepseek-chat': 0.002,
    'claude-3': 0.015,
    'qwen': 0.001,
    'llama': 0.001,
    'mistral': 0.001
  };
  
  const cost = costPer1kTokens[model] || 0.01;
  return (promptTokens + completionTokens) * cost / 1000;
} 