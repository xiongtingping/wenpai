import { checkGeminiAvailability } from './apiProxy';

interface ContentGenerationParams {
  originalContent: string;
  targetPlatforms: string[];
  platformSettings: Record<string, unknown>;
}

// Generation step interface
interface GenerationStep {
  name: string;
  description: string;
  status: "waiting" | "processing" | "completed" | "error";
}

// Platform generation status interface
interface PlatformGenerationStatus {
  platform: string;
  platformName: string;
  steps: GenerationStep[];
}

// API response type
export interface AIApiResponse {
  content: string;
  source: "ai" | "simulation";
  error?: string;
}

// API status type for monitoring
export interface ApiStatus {
  available: boolean;
  lastChecked: Date;
  errorMessage?: string;
  responseTime?: number;
}

// API timeout settings (in milliseconds)
const MAX_RETRIES = 2;
const RETRY_DELAY = 1000; // 1 second

// API status monitoring
let currentApiStatus: ApiStatus = {
  available: true,
  lastChecked: new Date()
};

// API provider selection - toggle between 'openai' and 'gemini'
// Set default to Gemini since we want to implement Gemini API
let currentApiProvider: 'openai' | 'gemini' = 'gemini';

// Platform style definitions and prompts
export const platformStyles = {
  zhihu: {
    name: "çŸ¥ä¹",
    style: "ç†æ€§åˆ†æå‹",
    description: "æ³¨é‡é€»è¾‘åˆ†æå’Œä¸“ä¸šè§‚ç‚¹ï¼Œä½¿ç”¨ä¸°å¯Œçš„äº‹å®å’Œä¸“ä¸šæœ¯è¯­ã€‚",
    tone: "ä¸“ä¸šæ­£å¼",
    uniqueFeatures: ["é—®ç­”å½¢å¼", "åˆ†æ®µè®ºè¿°", "å¼•ç”¨æ•°æ®æ”¯æŒ"],
    prompt: "å°†ä»¥ä¸‹å†…å®¹æ”¹å†™ä¸ºé€‚åˆçŸ¥ä¹å¹³å°çš„åˆ†æå‹æ–‡ç« ï¼Œè¦æ±‚ï¼šæ˜ç¡®æå‡ºä¸€ä¸ªé—®é¢˜æˆ–è§‚ç‚¹ï¼›ç»“æ„ä¸ŠåŒ…å«å¼•è¨€ã€èƒŒæ™¯ã€æ ¸å¿ƒåˆ†æã€å¯¹æ¯”ã€æ€»ç»“äº”éƒ¨åˆ†ï¼›ç”¨ç†æ€§è¯­è¨€è¡¨è¾¾ï¼Œé€‚åº¦å¼•ç”¨æ•°æ®æˆ–ç ”ç©¶æ”¯æŒè®ºç‚¹ï¼›æ–‡é£æ­£å¼ï¼Œé¿å…å£è¯­åŒ–è¡¨è¾¾ã€‚"
  },
  douyin: {
    name: "æŠ–éŸ³è„šæœ¬",
    style: "çŸ­è§†é¢‘è„šæœ¬å‹",
    description: "é•œå¤´åˆ†é•œæç¤ºï¼Œç®€çŸ­æœ‰åŠ›çš„è¡¨è¾¾ï¼Œæ³¨é‡è§†è§‰å†²å‡»å’Œæƒ…ç»ªå…±é¸£ã€‚",
    tone: "æ´»æ³¼ç›´æ¥",
    uniqueFeatures: ["é•œå¤´æè¿°", "å¿«èŠ‚å¥", "å¸å¼•çœ¼çƒçš„å¼€åœº"],
    prompt: "è¯·æ ¹æ®å†…å®¹ï¼Œç¼–å†™ä¸€æ®µé€‚åˆæŠ–éŸ³çš„çŸ­è§†é¢‘è„šæœ¬ï¼Œè¦æ±‚ï¼šå†…å®¹æ—¶é•¿æ§åˆ¶åœ¨60ç§’å†…ã€ç”¨åˆ†é•œå¤´æˆ–ä¸‰æ®µå¼ç»“æ„ï¼ˆå¼•å­ â†’ è½¬æŠ˜ â†’ åè½¬/çˆ†ç‚¹ï¼‰ã€æ”¯æŒåŠ å…¥å­—å¹•æç¤ºè¯ï¼Œå¦‚ã€ŒBGMæ¸å¼ºã€ã€ã€Œç”»é¢åˆ‡æ¢ã€ã€é€‚å½“ä½¿ç”¨å£è¯­ã€ç½‘ç»œæµè¡Œè¯­ï¼Œå¢å¼ºä¼ æ’­åŠ›ã€‚"
  },
  wechat: {
    name: "å…¬ä¼—å·",
    style: "å¹²è´§æ•™ç¨‹å‹",
    description: "ç³»ç»ŸåŒ–çŸ¥è¯†è¾“å‡ºï¼Œç»“æ„æ¸…æ™°ï¼Œå†…å®¹å…¨é¢ä¸”æ·±å…¥ã€‚",
    tone: "ä¸“ä¸šæƒå¨",
    uniqueFeatures: ["æ¸…æ™°åˆ†çº§æ ‡é¢˜", "è¯¦å°½å†…å®¹", "å›¾æ–‡ç»“åˆæè¿°"],
    prompt: "å°†å†…å®¹æ”¹å†™ä¸ºå…¬ä¼—å·æ–‡ç« ï¼Œè¦æ±‚ï¼šä¸»é¢˜èšç„¦ã€ä¿¡æ¯å¯†åº¦é«˜ï¼Œé¿å…ç©ºæ´é™ˆè¿°ã€ä½¿ç”¨ç»“æ„æ¸…æ™°çš„å°æ ‡é¢˜ï¼ˆå¦‚ï¼šä¸€ã€äºŒã€ä¸‰ï¼‰ã€æ¯ä¸€æ®µè½æœ€å¥½ä»¥è¡ŒåŠ¨æŒ‡å¼•/å®æ“æŠ€å·§æ”¶å°¾ã€æœ€åæ€»ç»“æ•´ä½“æ¡†æ¶å¹¶ç»™äºˆè¡ŒåŠ¨å»ºè®®ã€‚"
  },
  weibo: {
    name: "å¾®åš",
    style: "æƒ…ç»ªå…±é¸£å‹",
    description: "ç®€çŸ­æœ‰åŠ›ï¼Œæƒ…æ„Ÿä¸°å¯Œï¼Œä½¿ç”¨å¤§é‡æ„Ÿå¹è¯å’Œæµè¡Œè¯­ã€‚",
    tone: "è½»æ¾éšæ„",
    uniqueFeatures: ["è¯é¢˜æ ‡ç­¾", "æƒ…æ„Ÿè¡¨è¾¾", "äº’åŠ¨æ€§å¼º"],
    prompt: "å°†å†…å®¹æ”¹å†™æˆå¾®åšæ ¼å¼ï¼Œä»¥ç¬¬ä¸€äººç§°å™è¿°ï¼Œè¥é€ ä»£å…¥æ„Ÿã€è¯­è¨€å¯Œæœ‰æƒ…ç»ªè¡¨è¾¾å’Œç”Ÿæ´»ç»†èŠ‚ï¼Œå¼•å‘å…±é¸£ã€å¯åŠ å…¥emojiã€æ®µè½æ–­å¥ã€æ„Ÿå¹å·ç­‰å¢å¼ºå¯è¯»æ€§ã€æœ€åå¼•å¯¼ç”¨æˆ·è¯„è®ºæˆ–ç‚¹èµã€‚"
  },
  video: {
    name: "è§†é¢‘å·",
    style: "çŸ­è§†é¢‘è„šæœ¬å‹",
    description: "ç±»ä¼¼æŠ–éŸ³ä½†æ›´ä¾§é‡å™äº‹å’Œå†…å®¹æ·±åº¦ï¼Œé€‚åˆç¨é•¿è§†é¢‘ã€‚",
    tone: "ç”ŸåŠ¨å™äº‹",
    uniqueFeatures: ["æ—¶é—´è½´æç¤º", "å™äº‹ç»“æ„", "äº’åŠ¨å¼•å¯¼"],
    prompt: "è¯·æ ¹æ®å†…å®¹ï¼Œç¼–å†™ä¸€æ®µé€‚åˆçŸ­è§†é¢‘çš„è„šæœ¬ï¼Œè¦æ±‚ï¼šå†…å®¹æ—¶é•¿æ§åˆ¶åœ¨60ç§’å†…ã€ç”¨åˆ†é•œå¤´æˆ–ä¸‰æ®µå¼ç»“æ„ï¼ˆå¼•å­ â†’ è½¬æŠ˜ â†’ åè½¬/çˆ†ç‚¹ï¼‰ã€æ”¯æŒåŠ å…¥å­—å¹•æç¤ºè¯ï¼Œå¦‚ã€ŒBGMæ¸å¼ºã€ã€ã€Œç”»é¢åˆ‡æ¢ã€ã€é€‚å½“ä½¿ç”¨å£è¯­ã€ç½‘ç»œæµè¡Œè¯­ï¼Œå¢å¼ºä¼ æ’­åŠ›ã€‚"
  },
  bilibili: {
    name: "Bç«™",
    style: "æ•™ç¨‹+æ•…äº‹æ··åˆå‹",
    description: "å…¼å…·ä¸“ä¸šæ€§å’Œè¶£å‘³æ€§ï¼Œé€‚åˆçŸ¥è¯†åˆ†äº«å’Œå¨±ä¹å†…å®¹ã€‚",
    tone: "è½»æ¾ä¸“ä¸š",
    uniqueFeatures: ["åˆ†Pæè¿°", "æ—¶é—´èŠ‚ç‚¹", "äºŒæ¬¡å…ƒæ–‡åŒ–å…ƒç´ "],
    prompt: "è¯·æ ¹æ®å†…å®¹ï¼Œç¼–å†™ä¸€æ®µé€‚åˆBç«™çš„çŸ­è§†é¢‘è„šæœ¬ï¼Œè¦æ±‚ï¼šå†…å®¹æ—¶é•¿æ§åˆ¶åœ¨60ç§’å†…ã€ç”¨åˆ†é•œå¤´æˆ–ä¸‰æ®µå¼ç»“æ„ï¼ˆå¼•å­ â†’ è½¬æŠ˜ â†’ åè½¬/çˆ†ç‚¹ï¼‰ã€æ”¯æŒåŠ å…¥å­—å¹•æç¤ºè¯ï¼Œå¦‚ã€ŒBGMæ¸å¼ºã€ã€ã€Œç”»é¢åˆ‡æ¢ã€ã€é€‚å½“ä½¿ç”¨å£è¯­ã€ç½‘ç»œæµè¡Œè¯­ï¼Œå¢å¼ºä¼ æ’­åŠ›ã€‚"
  },
  xiaohongshu: {
    name: "å°çº¢ä¹¦",
    style: "æƒ…ç»ªå…±é¸£+å¹²è´§æ··åˆå‹",
    description: "è§†è§‰ä¼˜å…ˆï¼Œäº²åˆ‡éšæ„ï¼Œæ³¨é‡ä¸ªäººä½“éªŒå’Œå®ç”¨å»ºè®®ã€‚",
    tone: "äº²å¯†éšæ„",
    uniqueFeatures: ["emojiä½¿ç”¨", "æ¸…å•æ ¼å¼", "ä¸ªäººåŒ–è¡¨è¾¾"],
    prompt: "å°†å†…å®¹æ”¹å†™æˆå°çº¢ä¹¦æ ¼å¼ï¼Œä»¥ç¬¬ä¸€äººç§°å™è¿°ï¼Œè¥é€ ä»£å…¥æ„Ÿã€è¯­è¨€å¯Œæœ‰æƒ…ç»ªè¡¨è¾¾å’Œç”Ÿæ´»ç»†èŠ‚ï¼Œå¼•å‘å…±é¸£ã€å¯åŠ å…¥emojiã€æ®µè½æ–­å¥ã€æ„Ÿå¹å·ç­‰å¢å¼ºå¯è¯»æ€§ã€æœ€åå¼•å¯¼ç”¨æˆ·è¯„è®ºæˆ–ç‚¹èµã€‚"
  }
};

/**
 * Get the current API status
 * @returns Current API status
 */
export function getApiStatus(): ApiStatus {
  return {...currentApiStatus};
}

/**
 * Set the current API provider
 * @param provider The API provider to use ('openai' or 'gemini')
 */
export function setApiProvider(provider: 'openai' | 'gemini'): void {
  currentApiProvider = provider;
  console.log(`API provider set to: ${provider}`);
}

/**
 * Get the current API provider
 * @returns Current API provider
 */
export function getApiProvider(): 'openai' | 'gemini' {
  return currentApiProvider;
}

/**
 * Check if the selected API is available
 * Updates the currentApiStatus
 */
export async function checkApiAvailability(): Promise<boolean> {
  // Use the appropriate API check based on provider
  if (currentApiProvider === 'gemini') {
    const startTime = Date.now();
    try {
      console.log('Checking Gemini API availability');
      
      // Use our proxy service to check Gemini API availability
      const proxyResponse = await checkGeminiAvailability();
      
      const responseTime = Date.now() - startTime;
      
      if (proxyResponse.success) {
        const data = proxyResponse.data as { available: boolean };
        currentApiStatus = {
          available: data.available,
          lastChecked: new Date(),
          responseTime
        };
        return data.available;
      } else {
        currentApiStatus = {
          available: false,
          lastChecked: new Date(),
          errorMessage: proxyResponse.error || 'Unknown error checking Gemini API',
          responseTime
        };
        return false;
      }
    } catch (error) {
      currentApiStatus = {
        available: false,
        lastChecked: new Date(),
        errorMessage: (error instanceof Error) ? error.message : 'Unknown error checking API availability',
        responseTime: Date.now() - startTime
      };
      return false;
    }
  } else {
    const startTime = Date.now();
    try {
      // Use our proxy service to check OpenAI API availability
      const response = await fetch('/api/status/openai');
      
      const responseTime = Date.now() - startTime;
      
      // Check if we get a JSON response
      const contentType = response.headers.get('content-type');
      if (!contentType || !contentType.includes('application/json')) {
        const textBody = await response.text();
        console.error('Non-JSON response from OpenAI status check:', textBody.substring(0, 500));
        
        currentApiStatus = {
          available: false,
          lastChecked: new Date(),
          errorMessage: `Unexpected non-JSON response: ${textBody.substring(0, 100)}...`,
          responseTime
        };
        return false;
      }
      
      const data = await response.json();
      
      if (response.ok) {
        currentApiStatus = {
          available: data.available,
          lastChecked: new Date(),
          responseTime
        };
        return data.available;
      } else {
        currentApiStatus = {
          available: false,
          lastChecked: new Date(),
          errorMessage: data.error || `API error: ${response.status}`,
          responseTime
        };
        return false;
      }
    } catch (error) {
      currentApiStatus = {
        available: false,
        lastChecked: new Date(),
        errorMessage: (error instanceof Error) ? error.message : 'Unknown error checking API availability',
        responseTime: Date.now() - startTime
      };
      return false;
    }
  }
}

/**
 * Main function to generate adapted content for different platforms
 * 
 * @param params Parameters for content generation
 * @returns Promise with generated content for each platform and source information
 */
export async function generateAdaptedContent(params: ContentGenerationParams): Promise<Record<string, AIApiResponse>> {
  try {
    // Log which API provider is being used
    console.log(`Using API provider: ${currentApiProvider}`);
    
    // Generate the steps for visualization (currently unused but kept for future UI integration)
    getGenerationSteps(params);
    
    // Process each platform in sequence, calling the selected API for each
    const results: Record<string, AIApiResponse> = {};
    
    for (const platformId of params.targetPlatforms) {
      const useBrandLibrary = (params.platformSettings[`${platformId}-brandLibrary`] as boolean) || false;
      const adaptedContent = await adaptContentForPlatform(params.originalContent, platformId, useBrandLibrary);
      results[platformId] = adaptedContent;
    }
    
    return results;
  } catch (error) {
    console.error('Content generation failed:', error);
    throw error;
  }
}

/**
 * Regenerate content for a single platform
 * 
 * @param params Parameters for content generation
 * @param platformId The specific platform to regenerate
 * @returns Promise with generated content for the specific platform and source information
 */
export async function regeneratePlatformContent(
  params: ContentGenerationParams,
  platformId: string
): Promise<Record<string, AIApiResponse>> {
  try {
    // Create a new params object with just the single platform
    const singlePlatformParams = {
      ...params,
      targetPlatforms: [platformId]
    };
    
    // Log which API provider is being used
    console.log(`Using API provider for regeneration: ${currentApiProvider}`);
    
    // Generate the steps for visualization (currently unused but kept for future UI integration)
    getGenerationSteps(singlePlatformParams);
    
    // Get platform-specific settings
    const useBrandLibrary = (params.platformSettings[`${platformId}-brandLibrary`] as boolean) || false;
    
    // Call the selected AI API to generate the content
    const adaptedContent = await adaptContentForPlatform(params.originalContent, platformId, useBrandLibrary);
    
    return { [platformId]: adaptedContent };
  } catch (error) {
    console.error(`Regeneration for ${platformId} failed:`, error);
    throw error;
  }
}

/**
 * Creates generation steps for visualization
 */
export function getGenerationSteps(params: ContentGenerationParams): PlatformGenerationStatus[] {
  // Generate steps for each platform
  const steps = params.targetPlatforms.map(platformId => {
    const platformStyle = platformStyles[platformId as keyof typeof platformStyles];
    
    return {
      platform: platformId,
      platformName: platformStyle?.name || platformId,
      steps: [
        {
          name: "å†…å®¹åˆ†æ",
          description: "åˆ†æåŸå§‹å†…å®¹çš„ä¸»é¢˜ã€ç»“æ„å’Œå…³é”®ç‚¹",
          status: "waiting" as const
        },
        {
          name: "å¹³å°é€‚é…",
          description: `åº”ç”¨${platformStyle?.name || platformId}çš„å†…å®¹é£æ ¼: ${platformStyle?.style || "æ ‡å‡†"}`,
          status: "waiting" as const
        },
        {
          name: "æ ¼å¼ä¼˜åŒ–",
          description: "åŸºäºå¹³å°ç‰¹æ€§ä¼˜åŒ–æ’ç‰ˆå’Œå‘ˆç°æ–¹å¼",
          status: "waiting" as const
        },
        {
          name: "ç”Ÿæˆå†…å®¹",
          description: "æ ¹æ®å¹³å°ç‰¹æ€§ç”Ÿæˆæœ€ç»ˆå†…å®¹",
          status: "waiting" as const
        }
      ]
    };
  });
  
  return steps;
}

/**
 * Adapts content specifically for the target platform
 * @param content Original content
 * @param platformId Target platform
 * @param useBrandLibrary Whether to use brand library resources
 * @returns Platform-adapted content with source information
 */
export async function adaptContentForPlatform(
  content: string, 
  platformId: string, 
  useBrandLibrary: boolean = false
): Promise<AIApiResponse> {
  // Get platform style info and prompt
  const platformPrompt = getPlatformPrompt(platformId);
  
  // Log the prompt being used
  console.log(`Using prompt for ${platformId}: ${platformPrompt}`);
  
  try {
    // Create system prompt
    const systemPrompt = `You are an expert content adapter specializing in creating platform-specific 
                         content optimized for different social media platforms. Your task is to adapt 
                         the user's content to the specified platform's style and format.`;
    
    // Create user prompt with platform requirements
    let userPrompt = `Please adapt the following content for the ${platformId} platform.
                     
                     Original content: ${content}
                     
                     Platform requirements: ${platformPrompt}`;
    
    // Add brand guidelines if requested
    if (useBrandLibrary) {
      userPrompt += `\n\nPlease incorporate brand voice and values in the adaptation. 
                    Ensure consistency in tone, terminology, and messaging across all platforms.
                    Maintain professional language standards and avoid public relations risks.`;
    }
    
    // Make the API call to the selected provider with retries, timeouts and error handling
    let result;
    
    if (currentApiProvider === 'gemini') {
      result = await callGeminiAPI(systemPrompt, userPrompt, platformId);
    } else {
      result = await callOpenAIAPI(systemPrompt, userPrompt, platformId);
    }
    
    return result;
  } catch (error) {
    console.error(`Error adapting content for ${platformId}:`, error);
    // Instead of silently falling back to simulation, we propagate the error
    // This will allow the UI to show the error state
    if (error instanceof Error) {
      return {
        content: "",
        source: "simulation",
        error: `API è°ƒç”¨å¤±è´¥: ${error.message}`
      };
    }
    return {
      content: "",
      source: "simulation",
      error: "API è°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ– API å¯†é’¥æœ‰æ•ˆæ€§"
    };
  }
}

/**
 * Make an API call to OpenAI through our backend proxy
 * @param systemPrompt The system prompt
 * @param userPrompt The user prompt
 * @param platformId The platform ID for context (used for fallback)
 * @returns Generated content with source information
 */
async function callOpenAIAPI(
  systemPrompt: string, 
  userPrompt: string, 
  platformId: string
): Promise<AIApiResponse> {
  const messages = [
    {
      role: "system",
      content: systemPrompt
    },
    {
      role: "user",
      content: userPrompt
    }
  ];
  
  // Retry loop implementation
  for (let attempt = 0; attempt <= MAX_RETRIES; attempt++) {
    try {
      console.log(`API call attempt ${attempt + 1}/${MAX_RETRIES + 1} for ${platformId}`);
      
      try {
        console.log('Sending request to OpenAI API via proxy...');
        
        const response = await fetch('/api/proxy/openai', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            messages,
            model: "gpt-3.5-turbo-0125", 
            temperature: 0.7
          })
        });
        
        console.log(`API response status: ${response.status}`);
        
        // Check if we have a JSON response
        const contentType = response.headers.get('content-type');
        if (!contentType || !contentType.includes('application/json')) {
          const textBody = await response.text();
          console.error('Non-JSON response from OpenAI API:', textBody.substring(0, 500));
          
          // Update API status
          currentApiStatus = {
            available: false,
            lastChecked: new Date(),
            errorMessage: `Unexpected non-JSON response: ${textBody.substring(0, 100)}...`
          };
          
          if (attempt < MAX_RETRIES) {
            console.log(`Will retry after error: Non-JSON response`);
            // Wait before retrying (with exponential backoff)
            await new Promise(resolve => setTimeout(resolve, RETRY_DELAY * Math.pow(2, attempt)));
            continue; // Try again
          }
          
          // Fall back to simulation when all retries fail
          return {
            content: generateSimulatedAIResponse(userPrompt, platformId),
            source: "simulation",
            error: `éJSONå“åº”: ${textBody.substring(0, 100)}...`
          };
        }
        
        // Now safe to parse JSON
        const data = await response.json();
        
        // Handle non-200 responses
        if (!response.ok) {
          // Get response body for more error details
          console.error('Error response body:', JSON.stringify(data));
          
          const errorMessage = data.error || `API error: ${response.status}`;
          
          // Update API status
          currentApiStatus = {
            available: false,
            lastChecked: new Date(),
            errorMessage: errorMessage
          };
          
          if (attempt < MAX_RETRIES) {
            console.log(`Will retry after error: ${errorMessage}`);
            // Wait before retrying (with exponential backoff)
            await new Promise(resolve => setTimeout(resolve, RETRY_DELAY * Math.pow(2, attempt)));
            continue; // Try again
          }
          
          console.error(`API error after ${attempt + 1} attempts:`, errorMessage);
          
          // Fall back to simulation when all retries fail
          return {
            content: generateSimulatedAIResponse(userPrompt, platformId),
            source: "simulation",
            error: errorMessage
          };
        }
        
        // Update API status to available
        currentApiStatus = {
          available: true,
          lastChecked: new Date()
        };
        
        // Process successful response
        console.log('Processing successful API response');
        
        if (!data.success) {
          throw new Error(data.error || 'API proxy returned an unsuccessful response');
        }
        
        return {
          content: data.data.choices[0].message.content,
          source: "ai" // Indicate this content came from the AI
        };
      } catch (fetchError) {
        console.error('Fetch error:', fetchError);
        
        // Re-throw other fetch errors
        throw fetchError;
      }
    } catch (error) {
      console.error(`API call error on attempt ${attempt + 1}:`, error);
      
      // Update API status
      currentApiStatus = {
        available: false,
        lastChecked: new Date(),
        errorMessage: error instanceof Error ? error.message : 'Unknown error'
      };
      
      // For the last attempt, fall back to simulation
      if (attempt >= MAX_RETRIES) {
        return {
          content: generateSimulatedAIResponse(userPrompt, platformId),
          source: "simulation",
          error: error instanceof Error ? error.message : "API è°ƒç”¨å¤±è´¥"
        };
      }
      
      // Wait before next retry (with exponential backoff)
      await new Promise(resolve => setTimeout(resolve, RETRY_DELAY * Math.pow(2, attempt)));
    }
  }
  
  // This should never be reached due to the return in the final attempt, but TypeScript requires it
  return {
    content: generateSimulatedAIResponse(userPrompt, platformId),
    source: "simulation",
    error: "æ‰€æœ‰ API é‡è¯•å°è¯•å¤±è´¥ï¼Œå·²ä½¿ç”¨æ¨¡æ‹Ÿå†…å®¹"
  };
}

/**
 * Make an API call to Gemini through the backend proxy
 * @param systemPrompt The system prompt
 * @param userPrompt The user prompt
 * @param platformId The platform ID for context (used for fallback)
 * @returns Generated content with source information
 */
async function callGeminiAPI(
  systemPrompt: string, 
  userPrompt: string, 
  platformId: string
): Promise<AIApiResponse> {
  // Combine system and user prompts for Gemini
  const fullPrompt = `${systemPrompt}\n\n${userPrompt}`;
  
  // Retry loop implementation
  for (let attempt = 0; attempt <= MAX_RETRIES; attempt++) {
    try {
      console.log(`Gemini API call attempt ${attempt + 1}/${MAX_RETRIES + 1} for ${platformId}`);
      
      try {
        console.log('Sending request to Gemini API via proxy...');
        
        const response = await fetch('/api/proxy/gemini', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            prompt: fullPrompt
          })
        });
        
        console.log(`Gemini API response status: ${response.status}`);
        
        // Check if we have a JSON response
        const contentType = response.headers.get('content-type');
        if (!contentType || !contentType.includes('application/json')) {
          const textBody = await response.text();
          console.error('Non-JSON response from Gemini API:', textBody.substring(0, 500));
          
          // Update API status
          currentApiStatus = {
            available: false,
            lastChecked: new Date(),
            errorMessage: `Unexpected non-JSON response: ${textBody.substring(0, 100)}...`
          };
          
          if (attempt < MAX_RETRIES) {
            console.log(`Will retry after error: Non-JSON response`);
            // Wait before retrying (with exponential backoff)
            await new Promise(resolve => setTimeout(resolve, RETRY_DELAY * Math.pow(2, attempt)));
            continue; // Try again
          }
          
          // Fall back to simulation when all retries fail
          return {
            content: generateSimulatedAIResponse(userPrompt, platformId),
            source: "simulation",
            error: `éJSONå“åº”: ${textBody.substring(0, 100)}...`
          };
        }
        
        // Now safe to parse JSON
        const data = await response.json();
        
        // Handle non-200 responses
        if (!response.ok) {
          // Get response body for more error details
          console.error('Error response body:', JSON.stringify(data));
          
          const errorMessage = data.error || `API error: ${response.status}`;
          
          // Update API status
          currentApiStatus = {
            available: false,
            lastChecked: new Date(),
            errorMessage: errorMessage
          };
          
          if (attempt < MAX_RETRIES) {
            console.log(`Will retry after error: ${errorMessage}`);
            // Wait before retrying (with exponential backoff)
            await new Promise(resolve => setTimeout(resolve, RETRY_DELAY * Math.pow(2, attempt)));
            continue; // Try again
          }
          
          console.error(`Gemini API error after ${attempt + 1} attempts:`, errorMessage);
          
          // Fall back to simulation when all retries fail
          return {
            content: generateSimulatedAIResponse(userPrompt, platformId),
            source: "simulation",
            error: errorMessage
          };
        }
        
        // Update API status to available
        currentApiStatus = {
          available: true,
          lastChecked: new Date()
        };
        
        // Process successful response
        console.log('Processing successful Gemini API response');
        
        if (!data.success) {
          throw new Error(data.error || 'API proxy returned an unsuccessful response');
        }
        
        // Extract content from Gemini API response format via proxy
        const content = data.data.candidates && 
                        data.data.candidates[0] && 
                        data.data.candidates[0].content && 
                        data.data.candidates[0].content.parts &&
                        data.data.candidates[0].content.parts[0] &&
                        data.data.candidates[0].content.parts[0].text || "";
        
        return {
          content: content,
          source: "ai" // Indicate this content came from the AI
        };
      } catch (fetchError) {
        console.error('Fetch error:', fetchError);
        
        // Re-throw other fetch errors
        throw fetchError;
      }
    } catch (error) {
      console.error(`Gemini API call error on attempt ${attempt + 1}:`, error);
      
      // Update API status
      currentApiStatus = {
        available: false,
        lastChecked: new Date(),
        errorMessage: error instanceof Error ? error.message : 'Unknown error'
      };
      
      // For the last attempt, fall back to simulation
      if (attempt >= MAX_RETRIES) {
        return {
          content: generateSimulatedAIResponse(userPrompt, platformId),
          source: "simulation",
          error: error instanceof Error ? error.message : "API è°ƒç”¨å¤±è´¥"
        };
      }
      
      // Wait before next retry (with exponential backoff)
      await new Promise(resolve => setTimeout(resolve, RETRY_DELAY * Math.pow(2, attempt)));
    }
  }
  
  // This should never be reached due to the return in the final attempt, but TypeScript requires it
  return {
    content: generateSimulatedAIResponse(userPrompt, platformId),
    source: "simulation",
    error: "æ‰€æœ‰ API é‡è¯•å°è¯•å¤±è´¥ï¼Œå·²ä½¿ç”¨æ¨¡æ‹Ÿå†…å®¹"
  };
}

/**
 * Generate a simulated AI response for testing when API is unavailable
 * This is only for development/testing purposes and should be removed in production
 * @param prompt The user prompt
 * @param platformId The platform ID
 * @returns Simulated AI response
 */
function generateSimulatedAIResponse(prompt: string, platformId: string): string {
  // Extract the original content from the prompt
  const contentMatch = prompt.match(/Original content: ([\s\S]+?)(?=Platform requirements:|$)/);
  const originalContent = contentMatch ? contentMatch[1].trim() : "Sample content";
  
  // Get platform style (currently unused but kept for future enhancement)
  // const platformStyle = platformStyles[platformId as keyof typeof platformStyles];
  
  // Generate platform-specific simulated response
  // NOTE: In production this would be completely replaced by the OpenAI response
  
  let adaptedContent = '';
  const contentLines = originalContent.split('\n');
  const title = contentLines[0] || 'Sample Title';
  const body = contentLines.slice(1).join('\n') || 'Sample content for the body of the post.';
  
  switch (platformId) {
    case 'zhihu': {
      adaptedContent = `# ${title}

## å¼•è¨€

è¿™æ˜¯ä¸€ä¸ªå€¼å¾—æ·±å…¥æ¢è®¨çš„è¯é¢˜ã€‚${body.substring(0, 100)}...

## èƒŒæ™¯åˆ†æ

åœ¨æ¢è®¨è¿™ä¸ªé—®é¢˜ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£ä¸€äº›åŸºæœ¬èƒŒæ™¯ã€‚${body.substring(100, 250) || body}

## æ ¸å¿ƒè§‚ç‚¹

æ ¹æ®æˆ‘çš„åˆ†æï¼Œè¿™ä¸ªé—®é¢˜å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢æ¥çœ‹å¾…ï¼š

1. ${body.substring(0, 50) || "ç¬¬ä¸€ä¸ªé‡è¦è§‚ç‚¹..."}
2. ${body.substring(50, 100) || "ç¬¬äºŒä¸ªé‡è¦è§‚ç‚¹..."}
3. ${body.substring(100, 150) || "ç¬¬ä¸‰ä¸ªé‡è¦è§‚ç‚¹..."}

## æ•°æ®æ”¯æŒ

ç ”ç©¶è¡¨æ˜ï¼Œ${body.substring(150, 250) || "ç›¸å…³æ•°æ®æ˜¾ç¤º..."}

## æ€»ç»“

ç»¼åˆä»¥ä¸Šåˆ†æï¼Œ${body.substring(0, 100) || "æˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®º..."}

å¸Œæœ›è¿™ä¸ªåˆ†æå¯¹ä½ æœ‰æ‰€å¸®åŠ©ã€‚å¦‚æœæœ‰é—®é¢˜ï¼Œæ¬¢è¿åœ¨è¯„è®ºåŒºè®¨è®ºã€‚`;
      break;
    }
    
    case 'xiaohongshu': {
      adaptedContent = `âœ¨ ${title} âœ¨

å¤§å®¶å¥½ï¼ä»Šå¤©æƒ³å’Œå¤§å®¶åˆ†äº«ä¸€ä¸ªè¶…æ£’çš„å¿ƒå¾—ï¼ğŸ˜Š

ã€æˆ‘çš„ä½“éªŒã€‘
${body.substring(0, 150) || "æˆ‘æœ€è¿‘çš„ä½“éªŒçœŸçš„å¤ªæ£’äº†..."}

ğŸŒŸ ä¸‰ä¸ªè¦ç‚¹ï¼š

1ï¸âƒ£ ${body.substring(0, 50) || "ç¬¬ä¸€ä¸ªé‡è¦å‘ç°..."}
2ï¸âƒ£ ${body.substring(50, 100) || "ç¬¬äºŒä¸ªé‡è¦ç»éªŒ..."}
3ï¸âƒ£ ${body.substring(100, 150) || "ç¬¬ä¸‰ä¸ªå¿…é¡»æ³¨æ„çš„ç‚¹..."}

ã€å°è´´å£«ã€‘
${body.substring(150, 250) || "è¿™é‡Œæœ‰ä¸€äº›å°æŠ€å·§åˆ†äº«ç»™å¤§å®¶..."}

å¤§å®¶æœ‰æ²¡æœ‰ç±»ä¼¼çš„ç»å†å‘¢ï¼Ÿæ¬¢è¿åœ¨è¯„è®ºåŒºç•™è¨€äº¤æµï¼â¤ï¸

#å¥½ç‰©åˆ†äº« #ç»éªŒå¿ƒå¾— #æ—¥å¸¸ç”Ÿæ´»`;
      break;
    }
    
    case 'wechat': {
      adaptedContent = `# ${title}

> å¯¼è¯»ï¼š${body.substring(0, 100) || "æœ¬æ–‡å°†ä¸ºæ‚¨è¯¦ç»†ä»‹ç»..."}

## ä¸€ã€æ ¸å¿ƒæ¦‚å¿µ

${body.substring(0, 200) || "è¿™éƒ¨åˆ†å†…å®¹è¯¦ç»†ä»‹ç»äº†æ ¸å¿ƒæ¦‚å¿µ..."}

å®æ“è¦ç‚¹ï¼šæŒæ¡è¿™äº›æ ¸å¿ƒæ¦‚å¿µï¼Œå°†å¸®åŠ©æ‚¨æ›´å¥½åœ°ç†è§£åç»­å†…å®¹ã€‚

## äºŒã€è¯¦ç»†åˆ†æ

### 1. èƒŒæ™¯äº†è§£

${body.substring(200, 400) || "äº†è§£èƒŒæ™¯ä¿¡æ¯å¯¹äºå…¨é¢æŠŠæ¡å¾ˆé‡è¦..."}

å®æ“è¦ç‚¹ï¼šæ”¶é›†ç›¸å…³èµ„æ–™ï¼Œå»ºç«‹çŸ¥è¯†æ¡†æ¶ã€‚

### 2. æ–¹æ³•æ­¥éª¤

${body.substring(400, 600) || "å…·ä½“çš„æ“ä½œæ­¥éª¤åŒ…æ‹¬ä»¥ä¸‹å‡ ç‚¹..."}

å®æ“è¦ç‚¹ï¼šæŒ‰ç…§æ­¥éª¤å®è·µï¼Œæ³¨æ„ç»†èŠ‚ã€‚

## ä¸‰ã€æ¡ˆä¾‹åˆ†äº«

${body.substring(600, 800) || "ä»¥ä¸‹æ˜¯å‡ ä¸ªå…¸å‹æ¡ˆä¾‹..."}

å®æ“è¦ç‚¹ï¼šä»æ¡ˆä¾‹ä¸­è·å–çµæ„Ÿï¼Œåº”ç”¨åˆ°è‡ªå·±çš„å®é™…æƒ…å†µã€‚

## æ€»ç»“ä¸å±•æœ›

${body.substring(0, 100) || "é€šè¿‡æœ¬æ–‡çš„ä»‹ç»ï¼Œç›¸ä¿¡æ‚¨å·²ç»å¯¹è¿™ä¸ªè¯é¢˜æœ‰äº†æ·±å…¥çš„äº†è§£..."}

å¦‚æœæ‚¨è§‰å¾—æœ¬æ–‡æœ‰å¸®åŠ©ï¼Œæ¬¢è¿ç‚¹èµã€åˆ†äº«ç»™æ›´å¤šéœ€è¦çš„æœ‹å‹ï¼`;
      break;
    }
    
    case 'weibo': {
      adaptedContent = `#${title}# 

åˆšåˆšä½“éªŒäº†ä¸€æŠŠï¼ŒçœŸçš„å¤ªèµäº†ï¼ğŸ’• ${body.substring(0, 50) || "æˆ‘çš„çœŸå®æ„Ÿå—..."}

è¯´å®è¯ï¼Œä¸€å¼€å§‹æˆ‘ä¹Ÿå¾ˆçŠ¹è±«ï¼Œä½†æ˜¯å°è¯•åå‘ç°çœŸçš„å¾ˆç®€å•ï¼

å°æŠ€å·§åˆ†äº«ï¼š
1. ${body.substring(50, 100) || "ç¬¬ä¸€ä¸ªå°æŠ€å·§..."}
2. ${body.substring(100, 150) || "ç¬¬äºŒä¸ªå°æŠ€å·§..."}

${body.substring(150, 200) || "æ›´å¤šæ„Ÿå—åˆ†äº«..."}

å¤§å®¶æœ‰ä»€ä¹ˆæƒ³æ³•ï¼Ÿæ¬¢è¿è¯„è®ºï¼ 

#çƒ­é—¨è¯é¢˜# #åˆ†äº«å¿ƒå¾—# #æ¯æ—¥ä¸€æ¢#`;
      break;
    }
    
    case 'douyin':
    case 'video': {
      adaptedContent = `ã€${title} - çŸ­è§†é¢‘è„šæœ¬ã€‘

ã€å¼€åœºã€‘å—¨ï¼Œå¤§å®¶å¥½ï¼ä»Šå¤©ç»™å¤§å®¶å¸¦æ¥è¶…å®ç”¨å†…å®¹ï¼

ã€åœºæ™¯1ã€‘
ã€é•œå¤´æè¿°ã€‘ç‰¹å†™é•œå¤´ï¼Œå±•ç¤ºä¸»é¢˜å†…å®¹
${body.substring(0, 100) || "å¼€åœºå†…å®¹æè¿°..."}
ã€BGMæ¸å¼ºã€‘

ã€åœºæ™¯2ã€‘
ã€é•œå¤´æè¿°ã€‘åˆ‡æ¢åˆ°æ¼”ç¤ºç”»é¢
${body.substring(100, 200) || "æ ¸å¿ƒå†…å®¹è®²è§£..."}
ã€ç”»é¢åˆ‡æ¢ã€‘

ã€åœºæ™¯3 - çˆ†ç‚¹ã€‘
ã€é•œå¤´æè¿°ã€‘ç‰¹å†™é•œå¤´ï¼Œå±•ç¤ºæƒŠå–œæ•ˆæœ
${body.substring(200, 300) || "æƒŠå–œå†…å®¹æˆ–åè½¬..."}
ã€BGMé«˜æ½®ã€‘

ã€ç»“æŸè¯­ã€‘
å¦‚æœè§‰å¾—æœ‰ç”¨ï¼Œè®°å¾—ç‚¹èµå…³æ³¨ï¼Œæˆ‘ä»¬ä¸‹æœŸå†è§ï¼
ã€å­—å¹•æç¤ºï¼šç‚¹èµ+å…³æ³¨ã€‘

#åˆ›ä½œç»éªŒ #çŸ­è§†é¢‘æŠ€å·§ #å†…å®¹åˆ†äº«`;
      break;
    }
    
    case 'bilibili': {
      adaptedContent = `ã€${title} - Bç«™è§†é¢‘è„šæœ¬ã€‘

0:00 å¼€åœºç™½
å¤§å®¶å¥½å•Šï¼Œæ¬¢è¿æ¥åˆ°æˆ‘çš„é¢‘é“ï¼ä»Šå¤©æˆ‘ä»¬æ¥èŠä¸€èŠ${title}!
${body.substring(0, 100) || "å¼€åœºå†…å®¹..."}

0:30 å†…å®¹æ¦‚è¿°
${body.substring(100, 200) || "æœ¬è§†é¢‘å°†ä¼šè®²è§£çš„ä¸»è¦å†…å®¹..."}

1:15 æ ¸å¿ƒå†…å®¹ç¬¬ä¸€éƒ¨åˆ†
${body.substring(200, 300) || "è¯¦ç»†è®²è§£ç¬¬ä¸€éƒ¨åˆ†..."}

2:45 æ ¸å¿ƒå†…å®¹ç¬¬äºŒéƒ¨åˆ†
${body.substring(300, 400) || "è¯¦ç»†è®²è§£ç¬¬äºŒéƒ¨åˆ†..."}

4:30 é‡ç‚¹æ€»ç»“
${body.substring(400, 500) || "å†…å®¹æ€»ç»“..."}

5:15 ç»“æŸè¯­
æ„Ÿè°¢è§‚çœ‹ï¼Œåˆ«å¿˜äº†ä¸€é”®ä¸‰è¿ï¼Œæˆ‘ä»¬ä¸‹æœŸå†è§ï¼

P.S. ç´ æå’Œå‚è€ƒèµ„æ–™å·²åœ¨è§†é¢‘ç®€ä»‹æ”¾å‡ºï¼Œæ¬¢è¿æŸ¥é˜…ï½

#å“”å“©å“”å“© #çŸ¥è¯†åˆ†äº« #ç»éªŒæ€»ç»“`;
      break;
    }
    
    default:
      adaptedContent = `å¹³å° ${platformId} çš„å†…å®¹ï¼š\n\n${originalContent}`;
  }
  
  return adaptedContent;
}

/**
 * Get platform-specific prompt for adaptation
 * @param platformId The platform ID
 * @returns Prompt for content adaptation
 */
function getPlatformPrompt(platformId: string): string {
  const platformStyle = platformStyles[platformId as keyof typeof platformStyles];
  return platformStyle?.prompt || "å°†å†…å®¹é€‚é…ä¸ºé€‚åˆè¯¥å¹³å°çš„æ ¼å¼å’Œé£æ ¼ã€‚";
}